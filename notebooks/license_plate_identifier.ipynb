{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8492edc-5bc3-4e3c-ab95-c339353dbb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: 1908x1080 @ 30.0 fps, 220 frames\n",
      "Scanning first 300 frames for the car...\n",
      "Car found at frame 68\n",
      "  100/220 (45.5%)\n",
      "  150/220 (68.2%)\n",
      "  200/220 (90.9%)\n",
      "Done. Output saved to /app/data/videos/output/plate_detection_output.avi\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plate Detection - Tail lights + license plate detection on full video\n",
    "Works under perspective distortion (uses minAreaRect as robust fallback)\n",
    "\n",
    "Pipeline per frame:\n",
    "  1. detect_tail_lights()      -> finds the two rear lights and their extreme points\n",
    "  2. detect_plate()            -> finds the license plate below the lights\n",
    "  3. Optical flow tracking     -> propagates detected points frame-to-frame\n",
    "  4. Periodic re-detection     -> corrects drift every N frames\n",
    "\n",
    "Output: annotated video with lights (cyan), extreme points (blue),\n",
    "        plate quadrilateral (green) drawn on every frame.\n",
    "\n",
    "Usage:\n",
    "    python plate_detection.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "CONFIG = {\n",
    "    # Minimum brightness (V channel) to detect rear lights\n",
    "    'V_LOWER': 210,\n",
    "\n",
    "    # Ignore lights below this fraction of frame height (avoids road reflections)\n",
    "    'MAX_Y_RATIO': 0.80,\n",
    "\n",
    "    # Minimum height/width ratio for light blobs (lights are taller than wide)\n",
    "    'MIN_VERTICAL_RATIO': 1.2,\n",
    "\n",
    "    # Minimum blob area as fraction of total frame area\n",
    "    'MIN_CONTOUR_AREA_RATIO': 0.00015,\n",
    "\n",
    "    # Vertical tolerance (px) when extracting the outermost edge point of each light\n",
    "    'Y_TOLERANCE': 5,\n",
    "\n",
    "    # Brightness range for the license plate (white/reflective surface)\n",
    "    'V_PLATE_LOW': 150,\n",
    "    'V_PLATE_HIGH': 240,\n",
    "\n",
    "    # How often (in frames) to run a full re-detection to correct tracking drift\n",
    "    'REDETECTION_INTERVAL': 30,\n",
    "\n",
    "    # How many frames to scan at the start before giving up on finding the car\n",
    "    'MAX_SEARCH_FRAMES': 300,\n",
    "}\n",
    "\n",
    "VIDEO_PATH  = '/app/data/videos/input/1.mp4'\n",
    "OUTPUT_PATH = '/app/data/videos/output/plate_detection_output.avi'\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TAIL LIGHT DETECTION\n",
    "# =========================\n",
    "\n",
    "def detect_tail_lights(frame, config):\n",
    "    \"\"\"\n",
    "    Detect the two rear tail lights and return their centers and outermost points.\n",
    "\n",
    "    Strategy:\n",
    "      - Threshold the V (brightness) channel to isolate bright blobs\n",
    "      - Filter by size, aspect ratio, and vertical position\n",
    "      - Split candidates into left/right clusters by x-coordinate\n",
    "      - Compute area-weighted centers for each cluster\n",
    "      - Find the outermost horizontal edge point at mid-height for each light\n",
    "\n",
    "    Args:\n",
    "        frame:  BGR image (numpy array)\n",
    "        config: detection parameters dict\n",
    "\n",
    "    Returns:\n",
    "        fari:                 [(cx_L, cy_L), (cx_R, cy_R)] sorted left to right\n",
    "        extremes:             {'SX': (x,y), 'DX': (x,y)} outermost edge points\n",
    "        contours_per_cluster: [[contours_L], [contours_R]]\n",
    "                              passed to detect_plate to avoid re-running the mask\n",
    "        or (None, None, None) if detection fails\n",
    "    \"\"\"\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # --- Brightness mask ---\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    V   = hsv[:, :, 2]\n",
    "\n",
    "    mask = cv2.inRange(V, config['V_LOWER'], 255)\n",
    "\n",
    "    # Close small gaps within blobs, remove isolated noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 9))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN,  kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    MIN_AREA = config['MIN_CONTOUR_AREA_RATIO'] * width * height\n",
    "    MAX_Y    = int(height * config['MAX_Y_RATIO'])\n",
    "\n",
    "    # --- Filter candidates ---\n",
    "    candidates = []\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area < MIN_AREA:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        if y > MAX_Y:               # too low in frame (likely road reflection)\n",
    "            continue\n",
    "        if w == 0 or h == 0:\n",
    "            continue\n",
    "        if w / h > 1.0:             # wider than tall -> not a rear light\n",
    "            continue\n",
    "        if h / w < config['MIN_VERTICAL_RATIO']:\n",
    "            continue\n",
    "\n",
    "        M = cv2.moments(c)\n",
    "        if M['m00'] == 0:\n",
    "            continue\n",
    "\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        candidates.append((c, cx, cy, area))\n",
    "\n",
    "    if len(candidates) < 2:\n",
    "        return None, None, None\n",
    "\n",
    "    # --- Split into left / right clusters by x-coordinate ---\n",
    "    candidates = sorted(candidates, key=lambda p: p[1])  # sort by cx\n",
    "    mid       = len(candidates) // 2\n",
    "    cluster_L = candidates[:mid]\n",
    "    cluster_R = candidates[mid:]\n",
    "\n",
    "    # --- Area-weighted centers ---\n",
    "    fari = []\n",
    "    for cluster in [cluster_L, cluster_R]:\n",
    "        total_area = sum(a for _, _, _, a in cluster)\n",
    "        cx_w = sum(cx * a for _, cx, _, a in cluster) / total_area\n",
    "        cy_w = sum(cy * a for _, _, cy, a in cluster) / total_area\n",
    "        fari.append((int(cx_w), int(cy_w)))\n",
    "\n",
    "    # --- Outermost edge points at mid-height ---\n",
    "    # For the left light we want the leftmost point; for the right, the rightmost.\n",
    "    extremes = {}\n",
    "    sides = [('SX', cluster_L, False), ('DX', cluster_R, True)]\n",
    "\n",
    "    for name, cluster, want_rightmost in sides:\n",
    "        y_mid    = int(np.mean([cy for _, _, cy, _ in cluster]))\n",
    "        edge_pts = []\n",
    "\n",
    "        for c, _, _, _ in cluster:\n",
    "            for px, py in c[:, 0, :]:\n",
    "                if abs(py - y_mid) <= config['Y_TOLERANCE']:\n",
    "                    edge_pts.append((px, py))\n",
    "\n",
    "        if not edge_pts:\n",
    "            # Fallback: use the bounding-box edge of the whole cluster\n",
    "            all_pts = np.vstack([c[:, 0, :] for c, _, _, _ in cluster])\n",
    "            idx = all_pts[:, 0].argmax() if want_rightmost else all_pts[:, 0].argmin()\n",
    "            extremes[name] = tuple(all_pts[idx])\n",
    "        else:\n",
    "            extremes[name] = (max if want_rightmost else min)(edge_pts, key=lambda p: p[0])\n",
    "\n",
    "    # Pass raw contours to detect_plate so it can compute bottom-y accurately\n",
    "    contours_per_cluster = [\n",
    "        [c for c, _, _, _ in cluster_L],\n",
    "        [c for c, _, _, _ in cluster_R],\n",
    "    ]\n",
    "\n",
    "    return fari, extremes, contours_per_cluster\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PLATE DETECTION\n",
    "# =========================\n",
    "\n",
    "def detect_plate(frame, extremes, contours_per_cluster, config):\n",
    "    \"\"\"\n",
    "    Detect the license plate in the region below the tail lights.\n",
    "\n",
    "    Strategy:\n",
    "      - Define ROI: horizontally between the two outer edge points,\n",
    "        vertically just below the bottom of the light blobs\n",
    "        (bottom-y is read from contours_per_cluster, not re-computed)\n",
    "      - Threshold ROI brightness to isolate the plate (bright white/silver)\n",
    "      - Apply morphological gradient + Otsu threshold to find edges\n",
    "      - Fit minAreaRect to the largest contour -> robust to perspective tilt\n",
    "\n",
    "    Args:\n",
    "        frame:                BGR image\n",
    "        extremes:             {'SX': (x,y), 'DX': (x,y)}\n",
    "        contours_per_cluster: [[contours_L], [contours_R]] from detect_tail_lights\n",
    "        config:               detection parameters dict\n",
    "\n",
    "    Returns:\n",
    "        plate_corners: {'TL': (x,y), 'TR': (x,y), 'BR': (x,y), 'BL': (x,y)}\n",
    "                       in global image coordinates, or None if detection fails\n",
    "    \"\"\"\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # --- Compute bottom-y directly from the light contours ---\n",
    "    # Using the contours already found avoids re-running the brightness mask\n",
    "    # and fixes the bug where fari_bottom_y was always empty.\n",
    "    fari_bottom_y = []\n",
    "    for cluster_contours in contours_per_cluster:\n",
    "        for c in cluster_contours:\n",
    "            _, y, _, h = cv2.boundingRect(c)\n",
    "            fari_bottom_y.append(y + h)\n",
    "\n",
    "    if not fari_bottom_y:\n",
    "        return None\n",
    "\n",
    "    y_base = max(fari_bottom_y)\n",
    "\n",
    "    # ROI horizontal bounds: between the two outermost edge points\n",
    "    x1 = max(0, extremes['SX'][0])\n",
    "    x2 = min(width - 1, extremes['DX'][0])\n",
    "\n",
    "    if x2 <= x1:\n",
    "        return None\n",
    "\n",
    "    # ROI vertical: start just below the lights, span ~18% of frame height\n",
    "    y1 = min(y_base + 20, height - 1)\n",
    "    y2 = min(y1 + int(0.18 * height), height)\n",
    "\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return None\n",
    "\n",
    "    # --- Brightness mask to isolate the plate blob ---\n",
    "    hsv_roi    = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    V_roi      = hsv_roi[:, :, 2]\n",
    "    mask_plate = cv2.inRange(V_roi, config['V_PLATE_LOW'], config['V_PLATE_HIGH'])\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 3))\n",
    "    mask_plate = cv2.morphologyEx(mask_plate, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask_plate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Keep only the largest blob (assumed to be the plate)\n",
    "    largest      = max(contours, key=cv2.contourArea)\n",
    "    mask_cluster = np.zeros_like(mask_plate)\n",
    "    cv2.drawContours(mask_cluster, [largest], -1, 255, -1)\n",
    "\n",
    "    ys, xs = np.where(mask_cluster > 0)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "\n",
    "    # Tight bounding box with padding to include plate edges\n",
    "    x_min, x_max = xs.min(), xs.max()\n",
    "    y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "    pad_x = int(0.25 * (x_max - x_min))\n",
    "    pad_y = int(0.40 * (y_max - y_min))\n",
    "\n",
    "    roi_x_min = max(0, x_min - pad_x)\n",
    "    roi_x_max = min(roi.shape[1], x_max + pad_x)\n",
    "    roi_y_min = max(0, y_min - pad_y)\n",
    "    roi_y_max = min(roi.shape[0], y_max + pad_y)\n",
    "\n",
    "    roi_plate = roi[roi_y_min:roi_y_max, roi_x_min:roi_x_max]\n",
    "    if roi_plate.size == 0:\n",
    "        return None\n",
    "\n",
    "    # --- Edge detection inside the plate crop ---\n",
    "    # CLAHE improves contrast for low-light / nighttime images\n",
    "    gray_plate = cv2.cvtColor(roi_plate, cv2.COLOR_BGR2GRAY)\n",
    "    clahe      = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray_plate = clahe.apply(gray_plate)\n",
    "    gray_plate = cv2.GaussianBlur(gray_plate, (7, 7), 0)\n",
    "\n",
    "    kernel_g  = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    gradient  = cv2.morphologyEx(gray_plate, cv2.MORPH_GRADIENT, kernel_g)\n",
    "\n",
    "    # Otsu threshold automatically adapts to the local brightness level\n",
    "    _, gradient_binary = cv2.threshold(\n",
    "        gradient, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    "    )\n",
    "\n",
    "    contours_grad, _ = cv2.findContours(\n",
    "        gradient_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    if not contours_grad:\n",
    "        return None\n",
    "\n",
    "    main_contour = max(contours_grad, key=cv2.contourArea)\n",
    "\n",
    "    # --- minAreaRect: robust to perspective tilt ---\n",
    "    # A standard bounding rect gives wrong corners when the plate is at an angle.\n",
    "    # minAreaRect fits a rotated rectangle and handles perspective correctly.\n",
    "    rect = cv2.minAreaRect(main_contour)\n",
    "    box  = cv2.boxPoints(rect)\n",
    "    box  = np.int32(box)\n",
    "\n",
    "    # Sort the 4 box points into TL / TR / BR / BL\n",
    "    # TL = smallest (x+y), BR = largest (x+y)\n",
    "    pts    = box.reshape(4, 2)\n",
    "    s      = pts.sum(axis=1)\n",
    "    idx_tl = np.argmin(s)\n",
    "    idx_br = np.argmax(s)\n",
    "    remaining = [i for i in range(4) if i not in [idx_tl, idx_br]]\n",
    "\n",
    "    if pts[remaining[0]][0] > pts[remaining[1]][0]:\n",
    "        idx_tr, idx_bl = remaining[0], remaining[1]\n",
    "    else:\n",
    "        idx_tr, idx_bl = remaining[1], remaining[0]\n",
    "\n",
    "    # Convert from roi_plate local coords -> global image coords\n",
    "    offset_x = roi_x_min + x1\n",
    "    offset_y = roi_y_min + y1\n",
    "\n",
    "    return {\n",
    "        'TL': (int(pts[idx_tl][0]) + offset_x, int(pts[idx_tl][1]) + offset_y),\n",
    "        'TR': (int(pts[idx_tr][0]) + offset_x, int(pts[idx_tr][1]) + offset_y),\n",
    "        'BR': (int(pts[idx_br][0]) + offset_x, int(pts[idx_br][1]) + offset_y),\n",
    "        'BL': (int(pts[idx_bl][0]) + offset_x, int(pts[idx_bl][1]) + offset_y),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FULL DETECTION (single frame)\n",
    "# =========================\n",
    "\n",
    "def detect_all(frame, config):\n",
    "    \"\"\"\n",
    "    Run lights + plate detection on a single frame.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'fari', 'extremes', 'plate_corners', 'contours_per_cluster'\n",
    "        or None if tail lights are not found\n",
    "    \"\"\"\n",
    "    fari, extremes, contours_per_cluster = detect_tail_lights(frame, config)\n",
    "\n",
    "    if fari is None:\n",
    "        return None\n",
    "\n",
    "    plate_corners = detect_plate(frame, extremes, contours_per_cluster, config)\n",
    "\n",
    "    return {\n",
    "        'fari':                 fari,\n",
    "        'extremes':             extremes,\n",
    "        'plate_corners':        plate_corners,\n",
    "        'contours_per_cluster': contours_per_cluster,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DRAW RESULTS ON FRAME\n",
    "# =========================\n",
    "\n",
    "def draw_detections(frame, fari, extremes, plate_corners, frame_idx, status):\n",
    "    \"\"\"\n",
    "    Draw lights, extreme points and plate corners on a copy of the frame.\n",
    "\n",
    "    Args:\n",
    "        frame:         BGR image\n",
    "        fari:          [(cx_L, cy_L), (cx_R, cy_R)] or None\n",
    "        extremes:      {'SX': (x,y), 'DX': (x,y)} or None\n",
    "        plate_corners: {'TL','TR','BR','BL'} or None\n",
    "        frame_idx:     current frame number (shown in overlay)\n",
    "        status:        string shown in top-right corner\n",
    "\n",
    "    Returns:\n",
    "        annotated copy of the frame\n",
    "    \"\"\"\n",
    "    vis = frame.copy()\n",
    "\n",
    "    # Light centers — cyan filled circles\n",
    "    if fari:\n",
    "        for pt in fari:\n",
    "            cv2.circle(vis, pt, 8, (0, 255, 255), -1)\n",
    "\n",
    "    # Outermost edge points — blue filled circles\n",
    "    if extremes:\n",
    "        for pt in extremes.values():\n",
    "            cv2.circle(vis, pt, 6, (255, 0, 0), -1)\n",
    "\n",
    "    # Plate quadrilateral — green outline + corner labels\n",
    "    if plate_corners:\n",
    "        poly = np.array(list(plate_corners.values()), dtype=np.int32)\n",
    "        cv2.polylines(vis, [poly], True, (0, 255, 0), 2)\n",
    "        for name, pt in plate_corners.items():\n",
    "            cv2.putText(vis, name, (pt[0] + 4, pt[1] - 4),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 1)\n",
    "\n",
    "    # Frame counter and tracking status\n",
    "    cv2.putText(vis, f\"Frame: {frame_idx}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    cv2.putText(vis, status, (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.65,\n",
    "                (0, 255, 0) if status == 'TRACKING' else (0, 200, 255), 2)\n",
    "\n",
    "    return vis\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN — PROCESS FULL VIDEO\n",
    "# =========================\n",
    "\n",
    "def process_video(video_path, output_path, config):\n",
    "    \"\"\"\n",
    "    Process an entire video:\n",
    "      - scan opening frames to find the car\n",
    "      - track lights + plate with optical flow\n",
    "      - re-detect every REDETECTION_INTERVAL frames to correct drift\n",
    "      - write annotated output video\n",
    "\n",
    "    Args:\n",
    "        video_path:  path to input video\n",
    "        output_path: path for the annotated output video\n",
    "        config:      detection + processing parameters dict\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps          = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width        = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height       = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Video: {width}x{height} @ {fps:.1f} fps, {total_frames} frames\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    out = cv2.VideoWriter(output_path,\n",
    "                          cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                          fps, (width, height))\n",
    "\n",
    "    # Optical flow parameters — used to propagate tracked points between frames\n",
    "    lk_params = dict(\n",
    "        winSize  = (21, 21),\n",
    "        maxLevel = 3,\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n",
    "    )\n",
    "\n",
    "    # ---- PHASE 1: scan until the car is found ----\n",
    "    print(f\"Scanning first {config['MAX_SEARCH_FRAMES']} frames for the car...\")\n",
    "\n",
    "    start_frame   = None\n",
    "    first_result  = None\n",
    "    first_frame   = None\n",
    "\n",
    "    for i in range(config['MAX_SEARCH_FRAMES']):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        result = detect_all(frame, config)\n",
    "\n",
    "        if result is not None:\n",
    "            start_frame  = i\n",
    "            first_result = result\n",
    "            first_frame  = frame.copy()\n",
    "            print(f\"Car found at frame {start_frame}\")\n",
    "            break\n",
    "\n",
    "        # Write waiting frames to output as-is\n",
    "        cv2.putText(frame, f\"Frame: {i} — waiting for car...\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)\n",
    "        out.write(frame)\n",
    "\n",
    "    if start_frame is None:\n",
    "        print(f\"Car not found in first {config['MAX_SEARCH_FRAMES']} frames. Exiting.\")\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        return\n",
    "\n",
    "    # ---- PHASE 2: build tracked_points dict for optical flow ----\n",
    "    # We track: left center, right center, and the 4 plate corners.\n",
    "    # All stored as float32 arrays of shape (1, 2) as required by calcOpticalFlowPyrLK.\n",
    "\n",
    "    def build_tracked_points(result):\n",
    "        \"\"\"Convert detection result into a flat dict of (1,2) float32 arrays.\"\"\"\n",
    "        pts = {}\n",
    "        pts['faro_L'] = np.array([result['fari'][0]], dtype=np.float32)\n",
    "        pts['faro_R'] = np.array([result['fari'][1]], dtype=np.float32)\n",
    "        if result['plate_corners']:\n",
    "            for k, v in result['plate_corners'].items():\n",
    "                pts[f'plate_{k}'] = np.array([v], dtype=np.float32)\n",
    "        return pts\n",
    "\n",
    "    tracked_points = build_tracked_points(first_result)\n",
    "    prev_gray      = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Write the detection frame\n",
    "    out.write(draw_detections(\n",
    "        first_frame,\n",
    "        first_result['fari'],\n",
    "        first_result['extremes'],\n",
    "        first_result['plate_corners'],\n",
    "        start_frame,\n",
    "        'DETECTED',\n",
    "    ))\n",
    "\n",
    "    # ---- PHASE 3: process remaining frames ----\n",
    "    frames_since_redetect = 0\n",
    "\n",
    "    for frame_idx in range(start_frame + 1, total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        status    = 'TRACKING'\n",
    "\n",
    "        # --- Optical flow: propagate all tracked points ---\n",
    "        new_tracked = {}\n",
    "        for name, pts in tracked_points.items():\n",
    "            new_pts, st, _ = cv2.calcOpticalFlowPyrLK(\n",
    "                prev_gray, curr_gray, pts, None, **lk_params\n",
    "            )\n",
    "            if st is not None and st[0][0] == 1:\n",
    "                new_tracked[name] = new_pts   # point successfully tracked\n",
    "            # If tracking fails for a point we simply drop it this frame\n",
    "\n",
    "        tracked_points = new_tracked\n",
    "\n",
    "        # --- Reconstruct fari and plate_corners from tracked points ---\n",
    "        fari = None\n",
    "        if 'faro_L' in tracked_points and 'faro_R' in tracked_points:\n",
    "            fari = [\n",
    "                tuple(tracked_points['faro_L'][0].astype(int)),\n",
    "                tuple(tracked_points['faro_R'][0].astype(int)),\n",
    "            ]\n",
    "\n",
    "        plate_corners = None\n",
    "        plate_keys    = ['plate_TL', 'plate_TR', 'plate_BR', 'plate_BL']\n",
    "        if all(k in tracked_points for k in plate_keys):\n",
    "            plate_corners = {\n",
    "                k.replace('plate_', ''): tuple(tracked_points[k][0].astype(int))\n",
    "                for k in plate_keys\n",
    "            }\n",
    "\n",
    "        # --- Periodic re-detection to correct drift ---\n",
    "        frames_since_redetect += 1\n",
    "        if frames_since_redetect >= config['REDETECTION_INTERVAL']:\n",
    "            fresh = detect_all(frame, config)\n",
    "            if fresh is not None:\n",
    "                # Re-detection succeeded: reset tracked points to fresh positions\n",
    "                tracked_points        = build_tracked_points(fresh)\n",
    "                fari                  = fresh['fari']\n",
    "                plate_corners         = fresh['plate_corners']\n",
    "                frames_since_redetect = 0\n",
    "                status                = 'RE-DETECTED'\n",
    "            else:\n",
    "                # Re-detection failed: keep using optical flow result\n",
    "                frames_since_redetect = 0\n",
    "                status                = 'TRACKING (redetect failed)'\n",
    "\n",
    "        # Extremes are not tracked by optical flow (they are edge points on contours).\n",
    "        # We only show them after a fresh detection; otherwise pass None.\n",
    "        extremes = fresh['extremes'] if status == 'RE-DETECTED' else None\n",
    "\n",
    "        out.write(draw_detections(frame, fari, extremes, plate_corners, frame_idx, status))\n",
    "\n",
    "        prev_gray = curr_gray.copy()\n",
    "\n",
    "        if frame_idx % 50 == 0:\n",
    "            pct = frame_idx / total_frames * 100\n",
    "            print(f\"  {frame_idx}/{total_frames} ({pct:.1f}%)\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Done. Output saved to {output_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ENTRY POINT\n",
    "# =========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_video(VIDEO_PATH, OUTPUT_PATH, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08545884-374f-4223-af08-d04a16a080b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
